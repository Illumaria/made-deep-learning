{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "seminar2_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jeJWounsqhpY"
      },
      "source": [
        "import numpy as np \n",
        "import _pickle as cPickle\n",
        "import gzip\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "D7VXd_waqhpl"
      },
      "source": [
        "# utility functions\n",
        "\n",
        "def one_hot_encoded(y, num_class):\n",
        "    n = y.shape[0]\n",
        "    onehot = np.zeros((n, num_class), dtype=\"int32\")\n",
        "    for i in range(n):\n",
        "        idx = y[i]\n",
        "        onehot[i][idx] = 1\n",
        "    return onehot\n",
        "\n",
        "\n",
        "def check_accuracy(y_true, y_pred):\n",
        "    return np.mean(y_pred == y_true)  # both are not one hot encoded\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "# l2 regularization\n",
        "def l2_reg(layers, lam=0.001):\n",
        "    reg_loss = 0.0\n",
        "    for layer in layers:\n",
        "        if hasattr(layer, 'W'):\n",
        "            reg_loss += 0.5 * lam * np.sum(layer.W * layer.W)\n",
        "    return reg_loss\n",
        "\n",
        "\n",
        "# l2 regularization grad\n",
        "def delta_l2_reg(layers, grads, lam=0.001):\n",
        "    for layer, grad in zip(layers, reversed(grads)):\n",
        "        if hasattr(layer, 'W'):\n",
        "            grad[0] += lam * layer.W\n",
        "    return grads"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gepenqShqhpn"
      },
      "source": [
        "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
        "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
        "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
        "    Adopted from https://github.com/ddtm/dl-course/\n",
        "    \"\"\"\n",
        "    fx = f(x) # evaluate function value at original point\n",
        "    grad = np.zeros_like(x)\n",
        "    # iterate over all indexes in x\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        # evaluate function at x+h\n",
        "        ix = it.multi_index\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h # increment by h\n",
        "        fxph = f(x) # evalute f(x + h)\n",
        "        x[ix] = oldval - h\n",
        "        fxmh = f(x) # evaluate f(x - h)\n",
        "        x[ix] = oldval # restore\n",
        "\n",
        "        # compute the partial derivative with centered formula\n",
        "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
        "        if verbose:\n",
        "            print (ix, grad[ix])\n",
        "        it.iternext() # step to next dimension\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aNH3LoYVqhpq"
      },
      "source": [
        "class ReLU():\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, X, mode):\n",
        "        self.X = X\n",
        "        return np.maximum(X, 0)\n",
        "\n",
        "    def backward(self, dout, mode):\n",
        "        self.gradInput = dout.copy()\n",
        "        self.gradInput[self.X <= 0] = 0\n",
        "        return self.gradInput, []"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsQb5bWBqhps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a99084-e40b-4311-e101-3dcf57717a33"
      },
      "source": [
        "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
        "relu = ReLU()\n",
        "f = lambda x: relu.forward(x, mode='train').sum(axis=1).sum()\n",
        "res = f(points)\n",
        "numeric_grads = eval_numerical_gradient(f, points)\n",
        "print(numeric_grads)\n",
        "inp_grad = np.ones(shape=(10, 12))\n",
        "grads = relu.backward(inp_grad, mode='train')[0]\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "t7FTFIqDqhpw"
      },
      "source": [
        "class Linear():\n",
        "    def __init__(self, in_size, out_size):\n",
        "        # Xavier init\n",
        "        self.W = np.random.randn(in_size, out_size) / np.sqrt(in_size + out_size/ 2.)\n",
        "        self.b = np.zeros((1, out_size))\n",
        "        self.params = [self.W, self.b]\n",
        "        self.gradW = None\n",
        "        self.gradB = None\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, X, mode):\n",
        "        self.X = X\n",
        "        out = self.X.dot(self.W) + self.b\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout, mode):\n",
        "        self.gradW = self.X.T.dot(dout)\n",
        "        self.gradB = np.mean(dout, axis=0)\n",
        "        self.gradInput = dout.dot(self.W.T)\n",
        "        return self.gradInput, [self.gradW, self.gradB]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-83NwDBqhpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff7326d-1771-42af-b34e-c2353629f20a"
      },
      "source": [
        "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
        "linear = Linear(12, 5)\n",
        "f = lambda x: linear.forward(x, mode='train').sum(axis=1).sum()\n",
        "res = f(points)\n",
        "numeric_grads = eval_numerical_gradient(f, points)\n",
        "print(numeric_grads)\n",
        "inp_grad = np.ones(shape=(10, 5))\n",
        "grads = linear.backward(inp_grad, mode='train')[0]\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247164 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247164 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]\n",
            " [ 1.87356212 -0.36764859  0.02878447  0.27186649 -0.06247165 -0.15072348\n",
            "  -0.78328876 -0.21824865 -0.04352113 -1.73074052 -0.23331544 -0.32597875]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Iz8ZiCXMqhp0"
      },
      "source": [
        "class CrossEntropyLoss(object):\n",
        "    def forward(self, X, y):\n",
        "        self.m = y.shape[0]\n",
        "        self.p = softmax(X)\n",
        "        cross_entropy = -np.log(self.p[range(self.m), y])\n",
        "        loss = np.sum(cross_entropy) / self.m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        dx = self.p.copy()\n",
        "        dx[range(self.m), y] -= 1\n",
        "        dx /= self.m\n",
        "        return dx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPOGFjZuqhp2"
      },
      "source": [
        "## NN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYkYJWpuqhp4"
      },
      "source": [
        "class NN:\n",
        "    def __init__(self, loss_func=CrossEntropyLoss(), mode = 'train'):\n",
        "        self.layers = []\n",
        "        self.params = []\n",
        "        self.loss_func = loss_func\n",
        "        self.grads = []\n",
        "        self.mode = mode\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        self.params.append(layer.params)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X, self.mode)\n",
        "        return X\n",
        "\n",
        "    def backward(self, dout):\n",
        "        self.clear_grad_param()\n",
        "        for layer in reversed(self.layers):\n",
        "            dout, grad = layer.backward(dout, self.mode)\n",
        "            self.grads.append(grad)\n",
        "        return self.grads\n",
        "\n",
        "    def train_step(self, X, y):\n",
        "        out = self.forward(X)\n",
        "        loss = self.loss_func.forward(out, y)\n",
        "        dout = self.loss_func.backward(out, y)\n",
        "        loss += l2_reg(self.layers)\n",
        "        grads = self.backward(dout)\n",
        "        grads = delta_l2_reg(self.layers, grads)\n",
        "        return loss, grads\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = self.forward(X)\n",
        "        return np.argmax(softmax(X), axis=1)\n",
        "\n",
        "    def dispGradParam():\n",
        "        print(self.grads)\n",
        "    \n",
        "    def clear_grad_param(self):\n",
        "        self.grads = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4JyB2D61qhp6"
      },
      "source": [
        "# SGD with momentum\n",
        "def update(velocity, params, grads, learning_rate=0.001, mu=0.9):\n",
        "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
        "        for i in range(len(g)):\n",
        "            v[i] = mu * v[i] + learning_rate * g[i]\n",
        "            p[i] -= v[i]\n",
        "\n",
        "# get minibatches\n",
        "def minibatch(X, y, minibatch_size):\n",
        "    n = X.shape[0]\n",
        "    minibatches = []\n",
        "    X, y = shuffle(X, y)\n",
        "\n",
        "    for i in range(0, n , minibatch_size):\n",
        "        X_batch = X[i:i + minibatch_size, ...]\n",
        "        y_batch = y[i:i + minibatch_size, ...]\n",
        "\n",
        "        minibatches.append((X_batch, y_batch))\n",
        "    return minibatches"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOfONQ0AqhqA"
      },
      "source": [
        "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9,\n",
        "          verbose=True, X_val=None, y_val=None, nesterov=True):\n",
        "    val_loss_epoch = []\n",
        "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
        "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
        "\n",
        "    c = 0 \n",
        "    for i in range(epoch):\n",
        "        loss_batch = []\n",
        "        val_loss_batch = []\n",
        "        velocity = []\n",
        "        for param_layer in net.params:\n",
        "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
        "            velocity.append(p)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Epoch {0}\".format(i + 1))\n",
        "\n",
        "        # iterate over mini batches\n",
        "        # for X_mini, y_mini in tqdm(minibatches):\n",
        "        for X_mini, y_mini in minibatches:\n",
        "            loss, grads = net.train_step(X_mini, y_mini)\n",
        "            loss_batch.append(loss)\n",
        "            update(velocity, net.params, grads,\n",
        "                            learning_rate=learning_rate, mu=mu)\n",
        "\n",
        "        # for X_mini_val, y_mini_val in tqdm(minibatches_val):\n",
        "        for X_mini_val, y_mini_val in minibatches_val:\n",
        "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
        "            val_loss_batch.append(val_loss)\n",
        "\n",
        "        # accuracy of model at end of epoch after all mini batch updates   \n",
        "        if verbose:\n",
        "            m_train = X_train.shape[0]\n",
        "            m_val = X_val.shape[0]\n",
        "            y_train_pred = np.array([], dtype=\"int64\")\n",
        "            y_val_pred = np.array([], dtype=\"int64\")\n",
        "\n",
        "            for i in range(0, m_train, minibatch_size):\n",
        "                X_tr = X_train[i:i + minibatch_size, : ]\n",
        "                y_tr = y_train[i:i + minibatch_size, ]\n",
        "                y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
        "\n",
        "            for i in range(0, m_val, minibatch_size):\n",
        "                X_va = X_val[i:i + minibatch_size, : ]\n",
        "                y_va = y_val[i:i + minibatch_size, ]\n",
        "                y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
        "\n",
        "            train_acc = check_accuracy(y_train, y_train_pred)\n",
        "            val_acc = check_accuracy(y_val, y_val_pred)\n",
        "\n",
        "            mean_train_loss = sum(loss_batch) / float(len(loss_batch))\n",
        "            mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
        "\n",
        "            # early stopping with patience = 5 on val loss\n",
        "            if len(val_loss_epoch) == 0:\n",
        "                val_loss_epoch.append(mean_val_loss)\n",
        "            else:\n",
        "                for j in val_loss_epoch[-5:]:\n",
        "                    if mean_val_loss > j:\n",
        "                        c += 1\n",
        "                    else:\n",
        "                        c = 0\n",
        "                if c > 5:\n",
        "                    print('Early stopping')\n",
        "                    return net\n",
        "                else:\n",
        "                    c = 0\n",
        "                    val_loss_epoch.append(mean_val_loss)    \n",
        "\n",
        "            print(\"Loss = {0} | Training Accuracy = {1} | \" \\\n",
        "                  \"Val Loss = {2} | Val Accuracy = {3}\".format(\n",
        "                mean_train_loss, train_acc, mean_val_loss, val_acc))\n",
        "    return net"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fad8x1_IqhqB",
        "outputId": "5e1e620b-8bcb-4076-8b1e-f401c5d7db24"
      },
      "source": [
        "# Get preprocessed training and validation data\n",
        "\n",
        "X_train = np.array([\n",
        "    [1, 2, 1, 2],\n",
        "    [2, 4, 2, 4],\n",
        "    [2, 1, 2, 1],\n",
        "    [4, 2, 4, 2],\n",
        "])\n",
        "\n",
        "y_train = np.array([0, 1, 0, 1])\n",
        "X_val = X_train.copy()\n",
        "y_val = y_train.copy()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "\n",
        "# define neural net\n",
        "model = NN()\n",
        "\n",
        "# add some layers\n",
        "model.add_layer(Linear(4, 100))\n",
        "model.add_layer(ReLU())\n",
        "model.add_layer(Linear(100, 100))\n",
        "model.add_layer(ReLU())\n",
        "model.add_layer(Linear(100, 2))\n",
        "\n",
        "model = train(model, X_train , y_train, minibatch_size=4, epoch=100,\n",
        "           learning_rate=0.1, X_val=X_val, y_val=y_val)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 4)\n",
            "(4, 4)\n",
            "Epoch 1\n",
            "Loss = 0.7416110089195137 | Training Accuracy = 0.5 | Val Loss = 0.6887695024737084 | Val Accuracy = 0.5\n",
            "Epoch 2\n",
            "Loss = 0.6887695024737084 | Training Accuracy = 0.5 | Val Loss = 0.6803123566123378 | Val Accuracy = 0.5\n",
            "Epoch 3\n",
            "Loss = 0.6803123566123378 | Training Accuracy = 0.5 | Val Loss = 0.6775512773446853 | Val Accuracy = 0.5\n",
            "Epoch 4\n",
            "Loss = 0.6775512773446853 | Training Accuracy = 0.5 | Val Loss = 0.6756828167244704 | Val Accuracy = 0.5\n",
            "Epoch 5\n",
            "Loss = 0.6756828167244704 | Training Accuracy = 0.5 | Val Loss = 0.6739691129059541 | Val Accuracy = 0.5\n",
            "Epoch 6\n",
            "Loss = 0.6739691129059541 | Training Accuracy = 0.5 | Val Loss = 0.6722872774552472 | Val Accuracy = 0.5\n",
            "Epoch 7\n",
            "Loss = 0.6722872774552472 | Training Accuracy = 0.5 | Val Loss = 0.6706007461942906 | Val Accuracy = 0.5\n",
            "Epoch 8\n",
            "Loss = 0.6706007461942906 | Training Accuracy = 0.5 | Val Loss = 0.6689200998478211 | Val Accuracy = 0.5\n",
            "Epoch 9\n",
            "Loss = 0.6689200998478211 | Training Accuracy = 0.5 | Val Loss = 0.667263697116572 | Val Accuracy = 0.5\n",
            "Epoch 10\n",
            "Loss = 0.667263697116572 | Training Accuracy = 0.5 | Val Loss = 0.665605615459113 | Val Accuracy = 0.5\n",
            "Epoch 11\n",
            "Loss = 0.665605615459113 | Training Accuracy = 0.5 | Val Loss = 0.6639289155162086 | Val Accuracy = 0.5\n",
            "Epoch 12\n",
            "Loss = 0.6639289155162086 | Training Accuracy = 0.5 | Val Loss = 0.6622612254298023 | Val Accuracy = 0.5\n",
            "Epoch 13\n",
            "Loss = 0.6622612254298023 | Training Accuracy = 0.5 | Val Loss = 0.6605839649826901 | Val Accuracy = 0.5\n",
            "Epoch 14\n",
            "Loss = 0.6605839649826901 | Training Accuracy = 0.5 | Val Loss = 0.6588659759884421 | Val Accuracy = 0.5\n",
            "Epoch 15\n",
            "Loss = 0.6588659759884421 | Training Accuracy = 0.5 | Val Loss = 0.6571573569068047 | Val Accuracy = 0.5\n",
            "Epoch 16\n",
            "Loss = 0.6571573569068047 | Training Accuracy = 0.5 | Val Loss = 0.6554391916391696 | Val Accuracy = 0.5\n",
            "Epoch 17\n",
            "Loss = 0.6554391916391696 | Training Accuracy = 0.5 | Val Loss = 0.6537314903067046 | Val Accuracy = 0.5\n",
            "Epoch 18\n",
            "Loss = 0.6537314903067046 | Training Accuracy = 0.5 | Val Loss = 0.6520162158831676 | Val Accuracy = 0.5\n",
            "Epoch 19\n",
            "Loss = 0.6520162158831676 | Training Accuracy = 0.5 | Val Loss = 0.6503041010399327 | Val Accuracy = 0.5\n",
            "Epoch 20\n",
            "Loss = 0.6503041010399327 | Training Accuracy = 0.5 | Val Loss = 0.648592680240379 | Val Accuracy = 0.5\n",
            "Epoch 21\n",
            "Loss = 0.648592680240379 | Training Accuracy = 0.5 | Val Loss = 0.6468808752292781 | Val Accuracy = 0.5\n",
            "Epoch 22\n",
            "Loss = 0.6468808752292781 | Training Accuracy = 0.5 | Val Loss = 0.6451682664705385 | Val Accuracy = 0.5\n",
            "Epoch 23\n",
            "Loss = 0.6451682664705385 | Training Accuracy = 0.5 | Val Loss = 0.6434545183064381 | Val Accuracy = 0.5\n",
            "Epoch 24\n",
            "Loss = 0.6434545183064381 | Training Accuracy = 0.5 | Val Loss = 0.6417397260776728 | Val Accuracy = 0.5\n",
            "Epoch 25\n",
            "Loss = 0.6417397260776728 | Training Accuracy = 0.5 | Val Loss = 0.6400229643859165 | Val Accuracy = 0.5\n",
            "Epoch 26\n",
            "Loss = 0.6400229643859165 | Training Accuracy = 0.5 | Val Loss = 0.6382761314796002 | Val Accuracy = 0.5\n",
            "Epoch 27\n",
            "Loss = 0.6382761314796002 | Training Accuracy = 0.5 | Val Loss = 0.6365509645396614 | Val Accuracy = 0.5\n",
            "Epoch 28\n",
            "Loss = 0.6365509645396614 | Training Accuracy = 0.5 | Val Loss = 0.6348233388121312 | Val Accuracy = 0.5\n",
            "Epoch 29\n",
            "Loss = 0.6348233388121312 | Training Accuracy = 0.5 | Val Loss = 0.6330928587814352 | Val Accuracy = 0.5\n",
            "Epoch 30\n",
            "Loss = 0.6330928587814352 | Training Accuracy = 0.5 | Val Loss = 0.6313592310146692 | Val Accuracy = 0.5\n",
            "Epoch 31\n",
            "Loss = 0.6313592310146692 | Training Accuracy = 0.5 | Val Loss = 0.6296221767262902 | Val Accuracy = 0.5\n",
            "Epoch 32\n",
            "Loss = 0.6296221767262902 | Training Accuracy = 0.5 | Val Loss = 0.6278814229968077 | Val Accuracy = 0.5\n",
            "Epoch 33\n",
            "Loss = 0.6278814229968077 | Training Accuracy = 0.5 | Val Loss = 0.6261367011012452 | Val Accuracy = 0.5\n",
            "Epoch 34\n",
            "Loss = 0.6261367011012452 | Training Accuracy = 0.5 | Val Loss = 0.6243877456092544 | Val Accuracy = 0.5\n",
            "Epoch 35\n",
            "Loss = 0.6243877456092544 | Training Accuracy = 0.5 | Val Loss = 0.6226342936895548 | Val Accuracy = 0.5\n",
            "Epoch 36\n",
            "Loss = 0.6226342936895548 | Training Accuracy = 0.5 | Val Loss = 0.6208851942166782 | Val Accuracy = 0.5\n",
            "Epoch 37\n",
            "Loss = 0.6208851942166782 | Training Accuracy = 0.5 | Val Loss = 0.6191394341641765 | Val Accuracy = 0.5\n",
            "Epoch 38\n",
            "Loss = 0.6191394341641765 | Training Accuracy = 0.5 | Val Loss = 0.6173571864883522 | Val Accuracy = 0.5\n",
            "Epoch 39\n",
            "Loss = 0.6173571864883522 | Training Accuracy = 0.5 | Val Loss = 0.6155831818547219 | Val Accuracy = 0.5\n",
            "Epoch 40\n",
            "Loss = 0.6155831818547219 | Training Accuracy = 0.5 | Val Loss = 0.6138043124421688 | Val Accuracy = 0.5\n",
            "Epoch 41\n",
            "Loss = 0.6138043124421688 | Training Accuracy = 0.5 | Val Loss = 0.6120194682865003 | Val Accuracy = 0.5\n",
            "Epoch 42\n",
            "Loss = 0.6120194682865003 | Training Accuracy = 0.5 | Val Loss = 0.6102283403469415 | Val Accuracy = 0.5\n",
            "Epoch 43\n",
            "Loss = 0.6102283403469415 | Training Accuracy = 0.5 | Val Loss = 0.6084306708308896 | Val Accuracy = 0.5\n",
            "Epoch 44\n",
            "Loss = 0.6084306708308896 | Training Accuracy = 0.5 | Val Loss = 0.6066262048025765 | Val Accuracy = 0.5\n",
            "Epoch 45\n",
            "Loss = 0.6066262048025765 | Training Accuracy = 0.5 | Val Loss = 0.6048146871357547 | Val Accuracy = 0.5\n",
            "Epoch 46\n",
            "Loss = 0.6048146871357547 | Training Accuracy = 0.5 | Val Loss = 0.6029958622754155 | Val Accuracy = 0.5\n",
            "Epoch 47\n",
            "Loss = 0.6029958622754155 | Training Accuracy = 0.5 | Val Loss = 0.6011629677244751 | Val Accuracy = 0.5\n",
            "Epoch 48\n",
            "Loss = 0.6011629677244751 | Training Accuracy = 0.5 | Val Loss = 0.5989850576709697 | Val Accuracy = 0.5\n",
            "Epoch 49\n",
            "Loss = 0.5989850576709697 | Training Accuracy = 0.5 | Val Loss = 0.5971149478460249 | Val Accuracy = 0.5\n",
            "Epoch 50\n",
            "Loss = 0.5971149478460249 | Training Accuracy = 0.5 | Val Loss = 0.5952416719874436 | Val Accuracy = 0.5\n",
            "Epoch 51\n",
            "Loss = 0.5952416719874436 | Training Accuracy = 0.5 | Val Loss = 0.5933598238699278 | Val Accuracy = 0.5\n",
            "Epoch 52\n",
            "Loss = 0.5933598238699278 | Training Accuracy = 0.5 | Val Loss = 0.5914689516337609 | Val Accuracy = 0.5\n",
            "Epoch 53\n",
            "Loss = 0.5914689516337609 | Training Accuracy = 0.5 | Val Loss = 0.5895703301457851 | Val Accuracy = 0.5\n",
            "Epoch 54\n",
            "Loss = 0.5895703301457851 | Training Accuracy = 0.5 | Val Loss = 0.5876623207359127 | Val Accuracy = 0.5\n",
            "Epoch 55\n",
            "Loss = 0.5876623207359127 | Training Accuracy = 0.5 | Val Loss = 0.585742540130471 | Val Accuracy = 0.5\n",
            "Epoch 56\n",
            "Loss = 0.585742540130471 | Training Accuracy = 0.5 | Val Loss = 0.5838130119906509 | Val Accuracy = 0.5\n",
            "Epoch 57\n",
            "Loss = 0.5838130119906509 | Training Accuracy = 0.5 | Val Loss = 0.5818730978144194 | Val Accuracy = 0.5\n",
            "Epoch 58\n",
            "Loss = 0.5818730978144194 | Training Accuracy = 0.5 | Val Loss = 0.5799225138347813 | Val Accuracy = 0.5\n",
            "Epoch 59\n",
            "Loss = 0.5799225138347813 | Training Accuracy = 0.5 | Val Loss = 0.5779609841290819 | Val Accuracy = 0.5\n",
            "Epoch 60\n",
            "Loss = 0.5779609841290819 | Training Accuracy = 0.5 | Val Loss = 0.575988232350102 | Val Accuracy = 0.5\n",
            "Epoch 61\n",
            "Loss = 0.575988232350102 | Training Accuracy = 0.5 | Val Loss = 0.5740039815106588 | Val Accuracy = 0.5\n",
            "Epoch 62\n",
            "Loss = 0.5740039815106588 | Training Accuracy = 0.5 | Val Loss = 0.5720079539932955 | Val Accuracy = 0.5\n",
            "Epoch 63\n",
            "Loss = 0.5720079539932955 | Training Accuracy = 0.5 | Val Loss = 0.5699998715796779 | Val Accuracy = 0.5\n",
            "Epoch 64\n",
            "Loss = 0.5699998715796779 | Training Accuracy = 0.5 | Val Loss = 0.5679794554894041 | Val Accuracy = 0.5\n",
            "Epoch 65\n",
            "Loss = 0.5679794554894041 | Training Accuracy = 0.5 | Val Loss = 0.5659464264272621 | Val Accuracy = 0.5\n",
            "Epoch 66\n",
            "Loss = 0.5659464264272621 | Training Accuracy = 0.5 | Val Loss = 0.56390050463898 | Val Accuracy = 0.5\n",
            "Epoch 67\n",
            "Loss = 0.56390050463898 | Training Accuracy = 0.5 | Val Loss = 0.5618414099756756 | Val Accuracy = 0.5\n",
            "Epoch 68\n",
            "Loss = 0.5618414099756756 | Training Accuracy = 0.5 | Val Loss = 0.5597688619672537 | Val Accuracy = 0.5\n",
            "Epoch 69\n",
            "Loss = 0.5597688619672537 | Training Accuracy = 0.5 | Val Loss = 0.5576825799050178 | Val Accuracy = 0.5\n",
            "Epoch 70\n",
            "Loss = 0.5576825799050178 | Training Accuracy = 0.5 | Val Loss = 0.5555822829337814 | Val Accuracy = 0.5\n",
            "Epoch 71\n",
            "Loss = 0.5555822829337814 | Training Accuracy = 0.5 | Val Loss = 0.5534676901537869 | Val Accuracy = 0.5\n",
            "Epoch 72\n",
            "Loss = 0.5534676901537869 | Training Accuracy = 0.5 | Val Loss = 0.5513385207327501 | Val Accuracy = 0.5\n",
            "Epoch 73\n",
            "Loss = 0.5513385207327501 | Training Accuracy = 0.5 | Val Loss = 0.5491944940283681 | Val Accuracy = 0.5\n",
            "Epoch 74\n",
            "Loss = 0.5491944940283681 | Training Accuracy = 0.5 | Val Loss = 0.5470353297216427 | Val Accuracy = 0.5\n",
            "Epoch 75\n",
            "Loss = 0.5470353297216427 | Training Accuracy = 0.5 | Val Loss = 0.5448607479613817 | Val Accuracy = 0.5\n",
            "Epoch 76\n",
            "Loss = 0.5448607479613817 | Training Accuracy = 0.5 | Val Loss = 0.5426720567264488 | Val Accuracy = 0.5\n",
            "Epoch 77\n",
            "Loss = 0.5426720567264488 | Training Accuracy = 0.5 | Val Loss = 0.5404709297052435 | Val Accuracy = 0.5\n",
            "Epoch 78\n",
            "Loss = 0.5404709297052435 | Training Accuracy = 0.5 | Val Loss = 0.5382479076657434 | Val Accuracy = 0.5\n",
            "Epoch 79\n",
            "Loss = 0.5382479076657434 | Training Accuracy = 0.5 | Val Loss = 0.5360091512685659 | Val Accuracy = 0.5\n",
            "Epoch 80\n",
            "Loss = 0.5360091512685659 | Training Accuracy = 0.5 | Val Loss = 0.5337535995227453 | Val Accuracy = 0.5\n",
            "Epoch 81\n",
            "Loss = 0.5337535995227453 | Training Accuracy = 0.5 | Val Loss = 0.5314809779163969 | Val Accuracy = 0.5\n",
            "Epoch 82\n",
            "Loss = 0.5314809779163969 | Training Accuracy = 0.5 | Val Loss = 0.5292272008143308 | Val Accuracy = 0.5\n",
            "Epoch 83\n",
            "Loss = 0.5292272008143308 | Training Accuracy = 0.5 | Val Loss = 0.5269598040217005 | Val Accuracy = 0.5\n",
            "Epoch 84\n",
            "Loss = 0.5269598040217005 | Training Accuracy = 1.0 | Val Loss = 0.524614152196472 | Val Accuracy = 1.0\n",
            "Epoch 85\n",
            "Loss = 0.524614152196472 | Training Accuracy = 1.0 | Val Loss = 0.5222667883125806 | Val Accuracy = 1.0\n",
            "Epoch 86\n",
            "Loss = 0.5222667883125806 | Training Accuracy = 1.0 | Val Loss = 0.5199043091558648 | Val Accuracy = 1.0\n",
            "Epoch 87\n",
            "Loss = 0.5199043091558648 | Training Accuracy = 1.0 | Val Loss = 0.5175253048129731 | Val Accuracy = 1.0\n",
            "Epoch 88\n",
            "Loss = 0.5175253048129731 | Training Accuracy = 1.0 | Val Loss = 0.5151274530120801 | Val Accuracy = 1.0\n",
            "Epoch 89\n",
            "Loss = 0.5151274530120801 | Training Accuracy = 1.0 | Val Loss = 0.5127104663837593 | Val Accuracy = 1.0\n",
            "Epoch 90\n",
            "Loss = 0.5127104663837593 | Training Accuracy = 1.0 | Val Loss = 0.5102740958195118 | Val Accuracy = 1.0\n",
            "Epoch 91\n",
            "Loss = 0.5102740958195118 | Training Accuracy = 1.0 | Val Loss = 0.5078180986879571 | Val Accuracy = 1.0\n",
            "Epoch 92\n",
            "Loss = 0.5078180986879571 | Training Accuracy = 1.0 | Val Loss = 0.5053282132174303 | Val Accuracy = 1.0\n",
            "Epoch 93\n",
            "Loss = 0.5053282132174303 | Training Accuracy = 1.0 | Val Loss = 0.5019465213742278 | Val Accuracy = 1.0\n",
            "Epoch 94\n",
            "Loss = 0.5019465213742278 | Training Accuracy = 1.0 | Val Loss = 0.49936028002119487 | Val Accuracy = 1.0\n",
            "Epoch 95\n",
            "Loss = 0.49936028002119487 | Training Accuracy = 1.0 | Val Loss = 0.4968014091049477 | Val Accuracy = 1.0\n",
            "Epoch 96\n",
            "Loss = 0.4968014091049477 | Training Accuracy = 1.0 | Val Loss = 0.49459999894388784 | Val Accuracy = 1.0\n",
            "Epoch 97\n",
            "Loss = 0.49459999894388784 | Training Accuracy = 1.0 | Val Loss = 0.4917170607523443 | Val Accuracy = 1.0\n",
            "Epoch 98\n",
            "Loss = 0.4917170607523443 | Training Accuracy = 1.0 | Val Loss = 0.48903816777106734 | Val Accuracy = 1.0\n",
            "Epoch 99\n",
            "Loss = 0.48903816777106734 | Training Accuracy = 1.0 | Val Loss = 0.48635904367238086 | Val Accuracy = 1.0\n",
            "Epoch 100\n",
            "Loss = 0.48635904367238086 | Training Accuracy = 1.0 | Val Loss = 0.48363884399569446 | Val Accuracy = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nasalrYhqhqC"
      },
      "source": [
        "## Mnist training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBnh0t5wqhqD"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5WLrrZbqhqE"
      },
      "source": [
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0W1H-iRHqhqF"
      },
      "source": [
        "y = y.astype(np.int32)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
        "                                                  test_size=0.25,\n",
        "                                                  shuffle=True,\n",
        "                                                  random_state=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Nb7XAVXnqhqG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87iNgx8uqhqH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "8c47f275-3d21-46d9-ddb9-39448ea2ecfc"
      },
      "source": [
        "# visualize data\n",
        "def vis(img, label):\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(label)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "vis_idx = 0\n",
        "vis(X_train[vis_idx].reshape(-1, 28), y_train[vis_idx])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAHr0lEQVR4nO3dT4jU5x3H8edpbRKNBKttTdVoCB40p5YYAoHc0j1UTIiUbSUgG4/REjyZYC+mNKQEShJ78CJlIRdLLoLkkIMQCCX04B+QNIiVBAMlZoN4CFVj/PXS0obuPKM7O5nPzL5eF8Evv/k9wr55lnmc39Su6wqQ5zujXgAwP3FCKHFCKHFCKHFCKHFCKHFCKHFOkFrrr2qtf6u1fllr/Xut9YlRr4mFWzbqBbA4aq0/K6X8vpTyy1LKX0spPx7tihhU9T+EJkOt9S+llKNd1x0d9VpYHH6tnQC11u+WUraVUn5Ya71Qa/201vrHWuvyUa+NhRPnZFhbSvleKeUXpZQnSik/KaX8tJTym1EuisGIczL8899/Hu667h9d182VUv5QSvn5CNfEgMQ5Abquu1JK+bSU8r9vIHgzYcyJc3L8qZTy61rrj2qt3y+l7C+lnBjxmhiAo5TJ8dtSyg9KKedLKddKKX8upfxupCtiII5SIJRfayGUOCGUOCGUOCFU893aWqt3i2DIuq6r8/29nRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCiRNCefremHnhhRea89dff705P3/+fHP+yiuv9JzNzs42r2Vx2TkhlDghlDghlDghlDghlDghVPO7UjwaczjuuuuunrN+xxXbt29vzu+9994Frek/zpw503P2yCOPDPTazM+jMWHMiBNCiRNCiRNCiRNCiRNCiRNC+cjYEPQ7a2ydVU5PTy/2cu7Ihx9+ONL78192TgglTgglTgglTgglTgglTgglTgjlnHMBVq5c2Zy/8cYbzfnMzMwiruabTp8+3Zy/9tprzfm77767mMthAHZOCCVOCCVOCCVOCCVOCCVOCCVOCOW5tQtw+PDh5vz5558f2r2vXr3anG/durU5/+yzzxZzOSwCz62FMSNOCCVOCCVOCCVOCCVOCOUjY/PYtm1bc75z586h3fv69evN+VNPPdWcOyqZHHZOCCVOCCVOCCVOCCVOCCVOCCVOCLUkzzlfeuml5nz//v3N+Zo1awa6f+ssc2pqqnnt+++/P9C9GR92TgglTgglTgglTgglTgglTgglTgi1JB+Nefny5eZ80HPMfo+vbH0m0znm0uPRmDBmxAmhxAmhxAmhxAmhxAmhxAmhJvbznA8//HDP2d133z3Qa3/xxRfN+a5du5pzZ5ncDjsnhBInhBInhBInhBInhBInhBInhJrYc865ubmes5s3bw702suXLx/oergddk4IJU4IJU4IJU4IJU4IJU4INbFHKa3HX3799dcDvfaKFSua8/vuu2+g10/14IMPNuerVq1qzp988snmvN/XH7ZcvHixOT9y5Ehz3u947dy5c3e8pkHZOSGUOCGUOCGUOCGUOCGUOCGUOCHUxH4F4I4dO3rOjh071ry236MzL1y40Jz3O8+7dOlScz5M99xzT3P+4osv9pzt3r27ee2mTZsWtKYE169fb86np6d7zk6cODHQvX0FIIwZcUIocUIocUIocUIocUIocUKoif0859atW3vOhv0VgKM8x3zsscea8wMHDjTnTz/99GIuZ2z0+5nYsmVLz9mg55y92DkhlDghlDghlDghlDghlDghlDgh1MSecw7T+vXrm/PWmVgppXz00UeLuZxvePvtt5vzdevWDe3egzp16lTP2TvvvNO8ds+ePc158r+7FzsnhBInhBInhBInhBInhBInhBInhJrYc84zZ870nF27dq15bb9nu27YsKE5f+6555rzfp+pHFfvvfdec97vs6JfffVVz9kzzzzTvPbGjRvNeT/9fibOnj070OsvhJ0TQokTQokTQokTQokTQokTQk3sUUrrrfVbt24N9d6rV69uzh966KGes4sXLzavnZmZac7Xrl3bnA/i6NGjzflbb73VnE9NTTXnL7/8cs/Z5s2bm9cuW9b+Ue73tY391vbJJ58058Ng54RQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQteu63sNaew/H2AcffNCcP/roo0O9/8cff9xzdvny5ea1999/f3O+cePGhSzptvT7WNaVK1ea82Gewfazd+/e5vzIkSPf0kr+X9d1db6/t3NCKHFCKHFCKHFCKHFCKHFCKHFCqCV5zvnAAw8057Ozs815v3PQFStW3PGaGMzJkyeb82effbY573e+PEzOOWHMiBNCiRNCiRNCiRNCiRNCiRNCLclzzkEdP368OX/88ceb837PtV2q5ubmes5effXV5rX9npn7+eefL2hN3wbnnDBmxAmhxAmhxAmhxAmhxAmhHKUMwapVq5rzffv29ZwdOnRosZfzrbl582ZzfvDgweb8zTff7Dnr91jOceYoBcaMOCGUOCGUOCGUOCGUOCGUOCGUc04YMeecMGbECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaHECaFq13WjXgMwDzsnhBInhBInhBInhBInhBInhPoX+fdf3PkoABQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFUz9i2qqhqJ",
        "outputId": "6c2fc05d-7db7-4091-c267-6e30cfc6a087"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "\n",
        "# define neural net\n",
        "model = NN()\n",
        "\n",
        "# add some layers\n",
        "model.add_layer(Linear(X_train.shape[1], 100))\n",
        "model.add_layer(ReLU())\n",
        "model.add_layer(Linear(100, 100))\n",
        "model.add_layer(ReLU())\n",
        "model.add_layer(Linear(100, 10))\n",
        "\n",
        "model = train(model, X_train , y_train, minibatch_size=128, epoch=10,\n",
        "           learning_rate=0.001, X_val=X_val, y_val=y_val)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(52500, 784)\n",
            "(17500, 784)\n",
            "Epoch 1\n",
            "Loss = 0.9133596840510689 | Training Accuracy = 0.9224 | Val Loss = 0.3231424224867492 | Val Accuracy = 0.9127428571428572\n",
            "Epoch 2\n",
            "Loss = 0.31530895630364386 | Training Accuracy = 0.9393714285714285 | Val Loss = 0.3256227408821692 | Val Accuracy = 0.9261714285714285\n",
            "Epoch 3\n",
            "Loss = 0.2690384821553248 | Training Accuracy = 0.9488952380952381 | Val Loss = 0.3175217670584186 | Val Accuracy = 0.9364571428571429\n",
            "Epoch 4\n",
            "Loss = 0.24063136944324126 | Training Accuracy = 0.9558666666666666 | Val Loss = 0.2446275841940419 | Val Accuracy = 0.9410857142857143\n",
            "Epoch 5\n",
            "Loss = 0.22174070234890472 | Training Accuracy = 0.9631809523809524 | Val Loss = 0.22273211163441764 | Val Accuracy = 0.9466285714285714\n",
            "Epoch 6\n",
            "Loss = 0.2060316547647717 | Training Accuracy = 0.9659238095238095 | Val Loss = 0.21248556584705294 | Val Accuracy = 0.9479428571428572\n",
            "Epoch 7\n",
            "Loss = 0.1955436448219298 | Training Accuracy = 0.968704761904762 | Val Loss = 0.21049479748190056 | Val Accuracy = 0.9504571428571429\n",
            "Epoch 8\n",
            "Loss = 0.18651728123558245 | Training Accuracy = 0.9709523809523809 | Val Loss = 0.18655639248436376 | Val Accuracy = 0.9520571428571428\n",
            "Epoch 9\n",
            "Loss = 0.1779018972961856 | Training Accuracy = 0.9723428571428572 | Val Loss = 0.18293057945066585 | Val Accuracy = 0.9528571428571428\n",
            "Epoch 10\n",
            "Loss = 0.17068081107506636 | Training Accuracy = 0.9745904761904762 | Val Loss = 0.17997748470714026 | Val Accuracy = 0.9537142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "bPLSr2-CqhqL",
        "outputId": "d0da6bf4-11a2-4728-f019-96353e7093d3"
      },
      "source": [
        "# visualize prediction \n",
        "vis_idx = 1\n",
        "pred = model.predict(X_val[vis_idx])\n",
        "vis(X_val[vis_idx].reshape(-1, 28), pred[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "VODz-x2iakIr",
        "outputId": "44701fca-e6a0-44a8-fdb6-7d4eb49ee43e"
      },
      "source": [
        "# visualize first layer mean weights\n",
        "t = model.layers[0].W.mean(axis=1).reshape(28, 28)\n",
        "plt.matshow(t)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5dd78b6a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbw0lEQVR4nO3de5ScdZ0m8Ofb3dX3Tnc6nXQnoXNvQkICSWhASMSAIgEEZWWyZhxkzzrCrLIHHPGg7Nkjx3V2WUccnVmXOSAo7FEcHCOXWRCQiwGVQCfmfr93mr4lnU7f79/9IyUGzPu8TTpUFft7PufkpFNPv1W/fqvy7bfqdzN3h4iEKyvdDRCR9FIREAmcioBI4FQERAKnIiASOBUBkcClpQiY2Qoz22lme8zsa+loA2NmB8xss5ltMLO6DGjPw2bWYmZbTrqt3MxeMLPdyb/HZ1j77jGzhuQ53GBm16SxfdVm9rKZbTOzrWZ2e/L2jDiHpH0pOYeW6nECZpYNYBeAKwEcBvAmgFXuvi2lDSHM7ACAWnc/ku62AICZXQagC8Cj7r4gedu3AbS5+73JQjre3e/KoPbdA6DL3b+TjjadzMwmA5js7uvNrATAOgCfAvAfkAHnkLRvJVJwDtNxJXARgD3uvs/dBwD8DMAn09CODwx3XwOg7V03fxLAI8mvH8GJF01aRLQvY7h7o7uvT37dCWA7gKnIkHNI2pcS6SgCUwHUn/Tvw0jhDzxKDuB5M1tnZrekuzERKt29Mfl1E4DKdDYmwm1mtin5diFtb1dOZmYzACwGsBYZeA7f1T4gBedQHwye2jJ3XwLgagBfSl7uZiw/8Z4u08Z/3w9gNoBFABoB3Jfe5gBmVgzgFwDucPeOk7NMOIenaF9KzmE6ikADgOqT/n1W8raM4e4Nyb9bAPwSJ97CZJrm5HvJP76nbElze97B3ZvdfdjdRwA8iDSfQzNL4MR/sJ+4++rkzRlzDk/VvlSdw3QUgTcB1JjZTDPLBfAZAE+loR2nZGZFyQ9nYGZFAD4OYAs/Ki2eAnBz8uubATyZxrb8mT/+50q6AWk8h2ZmAB4CsN3dv3tSlBHnMKp9qTqHKe8dAIBkV8f3AGQDeNjd/y7ljYhgZrNw4rc/AOQA+Gm622dmjwFYDqACQDOAbwB4AsDjAKYBOAhgpbun5cO5iPYtx4nLWAdwAMCtJ73/TnX7lgF4FcBmACPJm+/GiffdaT+HpH2rkIJzmJYiICKZQx8MigRORUAkcCoCIoFTERAJnIqASODSWgQyeEguALVvrDK5fZncNiC17Uv3lUBGPxFQ+8Yqk9uXyW0DUti+dBcBEUmzMQ0WMrMVAL6PEyP/fuju97LvT5QWeH5V6dv/HmzvQaKscNSPl2PDNO8ZyKX51KJ2mjd0v3OS1nBHN7LHFb3977L8Hnp8ezf/WSyHn+vsrBGav/vooeM9yCn902POLuDLHxwaKKd5/2AOzRM5/PyPy+l7x7+7jw2gaPyfnpO2fn5+qgv4YL2GPj6JriBnkOad/flvf/3u5xYAxuX30uM7egtobln8+S3KHaB5z2Di7a+HOnqQM+6d5ysr5v6r86PPX9PhIbS3DdupMv6sE8nFQX6AkxYHMbOn2OIg+VWlqL3/s5H3OeKnbOPbJhZ00Xz9oWqaf+sCPjT87rU30PyGczfQfPUbtTRPlPXTvGwcLzJDw/zC7efnP0Tz2/f/Bc33tlbQvKqsg+ZXVu6g+U/38PPz/fN/RvOvbf80zc+reIvmL+86m+Yr5vF1bZ7dvIDm+SX8+b1gaj3N/9DIZ9SXFPD7v++cxyOzL1x/ODIby9sBLQ4i8v+BsRSBD8LiICIS433/YNDMbjGzOjOrG2znl7siknpjKQKjWhzE3R9w91p3r30vHwKKSGqMpQhk9OIgIjI6p9074O5DZnYbgOfwp8VBtrJjhoaz0NxREpnn5gzRx2w4Wkrzsyfz1aFah8bRvLCYf/ra0FdG839Z8QOa3/TG52neWs+7wL647EWaX/37L9J8UhnvXenvTdB8zjTeBbmk8ADNr1q8meY3vsTb/58vfonme3on0XxZzR6aH+rm5//zta/R/JcHz6N5eW43zQvzeBdnVVEnzX/Y8pHI7Mhg9O/n0y4CAODuzwB4Ziz3ISLppRGDIoFTERAJnIqASOBUBEQCpyIgEjgVAZHAjamL8D0/WNYIKoqj+0ovnbiPHr+tYzLNN+6YRvMfz/k5zQ9Mn0DzZ/fPp/ln9txK89rZB2meW82n6h4bLKJ5WQmfCtsUM87irxf9luZPN/BZdJ1DS2l+sIP3wyeK+FTbl47Mpfm0omM0P9TJp1L3DfH/Dk1F/PyVF/Lz//y+c2heVcbHAXQP8anyG7dOj8w6e56PzHQlIBI4FQGRwKkIiARORUAkcCoCIoFTERAJnIqASOBSOk6gNLcX106JnlPeNZwfmQHAlRP5arCzi1tpflfDCppvPVpF8+tmb6F5RYL38zb1837m+YV8tdz8LD7f/OWsGppfM5e3/4frl9Ec3fzlMn0R76ePXdI8wcdJtPfxJb+P9PBxFCV5fL2I66dsovlrbbNpvu8tvlrzpAq+WvMrC56g+ZXbr6P5FUui/388XdgXmelKQCRwKgIigVMREAmcioBI4FQERAKnIiASOBUBkcCNaWvy96r47Cpf9IPPReaJbN5PfN1k3o/7z1s/TPMJ4/i6722dvJ85keD7Inzz3Kdp/uU1n6F50Xg+Hz1u6/Le7XxfhMEqPl8fI3xXaAzy3xl5zXwcwEA5f34nzT7Kjx/KpvnwCG9fdRnfmn7/Ub7ewKqadTTPixnHsbp+Ec0nFvLX51UT6bYeeKIx+v7f/JufoGNn8ymfYF0JiARORUAkcCoCIoFTERAJnIqASOBUBEQCpyIgEriUrieQlz2MaSXRc86nFbTR46ck+Hz1GRX8+LbeQppPK+f3PzDC+6n/tbWW5kvn7aH5m/V834TCYt7P353gYz6ycvg4g8Uz6mm+eQ1fr6BqLe8nb1zKX2559/N++tZ/z+9/pCtB8/1r+b4Hw4v5ehBn5zfS/K6XV9K8pLKL5rfNeoXm33j6L2g+c3EDzaOMqQiY2QEAnQCGAQy5O/9fICIZ50xcCVzu7kfOwP2ISBroMwGRwI21CDiA581snZndciYaJCKpNda3A8vcvcHMJgF4wcx2uPuak78hWRxuAYDCquIxPpyInGljuhJw94bk3y0AfgngolN8zwPuXuvutfllfDVhEUm90y4CZlZkZiV//BrAxwHwNa1FJOOc9noCZjYLJ377AyfeVvzU3f+OHTNrYZF/a/W5kfn3911BHzNu3fqzSo/TfHIBzwdG+P2/sm0uzfOK+br286uaaL6tie97MNDMxzkg5qnM7uU1v/JNPo6g8yw+TiK3gzegdxJfryBmGAay+HIOGMnlef9cvl7DxHI+TqC5me8b8bH5O/jjx/yAC0t4P/+Orsk0L0lE7y3wL599Ds3b2k75BJz2ZwLuvg/A+ad7vIhkBnURigRORUAkcCoCIoFTERAJnIqASOBUBEQCl9L1BDqH87GmI7qv/Ttzf06Pv+mZ/0TzSfP5fO1NR6fQvKmez2cvnNBD8wnFPN/ZOonmvosPq07U8HXpB5sLaD7tOb4eQf4Bvu4/vJLGfeP57xTj2w6gfA//hrcu48fnN8cMNGjJo3FbPR/ROvfiQzQfcv7zb2ieSvM36qfT/NuLf0Hzx1v/bMDu24Y8+tzoSkAkcCoCIoFTERAJnIqASOBUBEQCpyIgEjgVAZHApXScQNdAHl5rmBmZD8bMt/70pW/QfH/3BJrva+H531zyCs2fbYxeCwEA8rL5hPe+Xj7hPXcuH+eQtb6E5tN+x+fL71nF1+WvfI2vZ5DNhxkg7zhfjyBmWwm0z+bPf3YvX6+g4AjPh4r5egbj9tIYu4rOovnCZfz12VU/jubnLOD7PjzadCnNb5y0LjJ7PSd6DIuuBEQCpyIgEjgVAZHAqQiIBE5FQCRwKgIigVMREAlcSscJ5OcM4pyKlsh8egGfz/6b1hqaH2zl+89Pn3iM5k81LKR5466JNM9t5zXVx/N+9L5C3k8+dTefb3/wKj4ffvbP+L4INsTbd3QBX68gly/bj56Y9QYSXbyfP6eH9/OXHBrkDYh5uQ/n8/u38XygRF7MxggLzjtI86XlfKDCj3d8iOaXT4hej0LrCYhIJBUBkcCpCIgETkVAJHAqAiKBUxEQCZyKgEjgUjpOoDSnD1dN2BqZf+t3n6DHr1xSR/P9TRU0zwLvh27v5v3gXsT76Se8xvuZW5bE1NwenvfxYRCo2Mh/vqxBPg5gOJ+PUxgu4D9fbx7Pczt4+/rH8+PL9vJ++JYL+HoJM37Jx6F01ZTSfOnN22ien8XHKXQO8H0Pnjh8Hs3vWvgczf9b3bWRWXPPzsgs9krAzB42sxYz23LSbeVm9oKZ7U7+HfPyFJFMNZq3Az8GsOJdt30NwIvuXgPgxeS/ReQDKLYIuPsaAO9eGOqTAB5Jfv0IgE+d4XaJSIqc7geDle7emPy6CQDfpE5EMtaYewfc3YHoT9zM7BYzqzOzus5jcRM8RCTVTrcINJvZZABI/h05NdDdH3D3WnevLRnPP70VkdQ73SLwFICbk1/fDODJM9McEUm12HECZvYYgOUAKszsMIBvALgXwONm9nkABwGsHM2DNfWV4L6tH4vMs/N5P3xhFp/PXTqum+a76/lHF/nFfL79FefuoPmrrXw9gqIG3g+e3R/Tzx+z7v/RBfz+B8bFrAcQ049fsYmfn+Mz+b4KIzEXgj1T+DiGCVtj9jVo5e0/uqSc5i1L+evv8NpFNL/yok00v27yZpr/8+YP0/zBA8tobln8548SWwTcfVVE9NHTekQRySgaNiwSOBUBkcCpCIgETkVAJHAqAiKBUxEQCVxK1xMoSgxiyZTDkfnVE3g/6v5+vu5/Vw9fd3/+jLdovnU3339+zV4+DqB4Ht/XIGcXn3HtvJsfndP5N4zfGbdvAD9+8uv8+LZz+Hz4iev5OI2G5UUxx8f0cxtv/7HzePsnv8LvfsKbfD2FwpVNNK/O58//kcHofQEAYFblEZof7+ev70tn7ovMns2LHuOhKwGRwKkIiARORUAkcCoCIoFTERAJnIqASOBUBEQCl9JxAnH+6ws30vyn1/xvmm85awrNt7WObSnEC5fx9QTWvjGX5kVlvJ+7byLvJ698g89376zm/dyImW+ee5yv629V/P5bF/NxAIXNMesl8IdHdyV/uWZ38uPfupovb2c5PD/WUkbz5zGP5vX7+TiXjy2O3pMDAPY21dD863Oejcxez+mJzHQlIBI4FQGRwKkIiARORUAkcCoCIoFTERAJnIqASOBSOk7AAYyQSfNfvvxX9PiHWi+j+Ru7Z/IGdPEfNzGhj+ZvHpxO85F8Pp99OI/X3JExPhuJLt4Pn3eUj1Non8XXCyhqjvv5+P0P5fM8t4OPg4Dz85d7PG6cBN/4IHd2B817jhTSPG4cwHW1f6D5+iPVNP/Lc9+k+Zef/lxk9lb79yIzXQmIBE5FQCRwKgIigVMREAmcioBI4FQERAKnIiASuJSOExgcyUZzb0lk/kTv+fT49l6+7vp/ufj/0nxv3ySaP759Cc1X1Gyn+a928/nkiW5+unuqeT/5QAnvBz+6iPfjY1zMegHb+DgBGP+d0c6XU8Dsr/6e5tlz59A8UcHX7W+bx/vx82uO07y7g7++4sypaaT5rg7++ruscg/NH33jUpqvuvx3kdkjD3ZFZrFXAmb2sJm1mNmWk267x8wazGxD8s81cfcjIplpNG8HfgxgxSlu/wd3X5T888yZbZaIpEpsEXD3NQDaUtAWEUmDsXwweJuZbUq+XeCb7IlIxjrdInA/gNkAFgFoBHBf1Dea2S1mVmdmdYPt0Ysdikh6nFYRcPdmdx929xEADwK4iHzvA+5e6+61iTL+6a2IpN5pFQEzm3zSP28AsCXqe0Uks8WOEzCzxwAsB1BhZocBfAPAcjNbhBNLBBwAcOtoHiwvawizSo5G5r/ewvvZ77zkOZo/uH8ZzVt3VNB88YW8n/Y3h2fT3OoLaF7cwMcBdE/h4wD6Y/YtKDrEjy9q5McXHBmg+cA4fv8lB3neftMlNC9s4ev+90zi6wEMFcfsq2A8L9jBxwn0V/BxGAdbymnO1tIAgF2H+b4Yi+ceoPmgR59/J48dWwTcfdUpbn4o7jgR+WDQsGGRwKkIiARORUAkcCoCIoFTERAJnIqASOBSup5AaU4vVozfHJn/fvwMenx1InqMAQBcf1b0fQPAj7ZcQfOcLN4PPLi5lOaY00vjtg4+YjJu34K8Y/zhS/fxx2+8hD9+84f574SsXt7PXbGe98NnDfMcxu+/r4LnOTGj0vs38iku/XP4OImsXD7OIyubP393LnyR5v+z7iqar6yso/nXX/10ZHasJ3qtAV0JiARORUAkcCoCIoFTERAJnIqASOBUBEQCpyIgEriUjhMY8GzUD0bPue5u4/PxaxJHaD4th3ek/+aCGpqvP8T3h88e4f3Uw2183f6RmPno2TH98O3n0Bid0/g4gMFxMfPtW/l6AHHz9Xsn8vb3T6AxSnfxl2PvJP74ceevv4rvu4BhfnzVRL5vwX1n/5zmf/X7v6Z5dSV//d6z8RM0//rS6EW//0dxdNt1JSASOBUBkcCpCIgETkVAJHAqAiKBUxEQCZyKgEjgUjpO4EhfMR7aRfZYH+Q1qXmY70//nXo+H/vqKr5HyoEyvi/Bkx2LaT635i2at/XyfvzcH/F16weK+fnp5c1HbznvJ88u5PnZf99P86Pn8/UWcjtojON8GAeG8/g4gcHxfL5/nHGTuvjjj/Dz/9/rr6V5Vsx6FddP2UTzuqLpND8+HD3OZpj8vteVgEjgVAREAqciIBI4FQGRwKkIiARORUAkcCoCIoFL6TiBvJwhzCxvi8ybcvn+9Gt7ZtO84Tjvp/5fDZfTvKCQ94Mjm/dTH+0ponl5AV8Yf38tr8lDRbyfOauPH28x6+bn5fN19/eu5Od3whZ+foob+PPr2bk0t5j5/v3l/OXcV8Hb51U0xt/OeYHmT7QuoflgT4Lm//Q63xfjxiXraP7q0eiBFl1D6yOz2CsBM6s2s5fNbJuZbTWz25O3l5vZC2a2O/k339lBRDLSaN4ODAH4irvPB/AhAF8ys/kAvgbgRXevAfBi8t8i8gETWwTcvdHd1ye/7gSwHcBUAJ8E8Ejy2x4B8Kn3q5Ei8v55Tx8MmtkMAIsBrAVQ6e6NyagJQOUZbZmIpMSoi4CZFQP4BYA73P0dU0Hc3QGc8lMXM7vFzOrMrG6gnW+YKSKpN6oiYGYJnCgAP3H31cmbm81scjKfDKDlVMe6+wPuXuvutbllfDVhEUm90fQOGICHAGx39++eFD0F4Obk1zcDePLMN09E3m+jGSewFMBNADab2YbkbXcDuBfA42b2eQAHAayMu6PB4Ww0do6LzO+o4fu3f2/3R2ne3sb76b944Ss0f2TXh2iOIV4zE/+Hrwew9yKeT9zE+7E7pvN9AfoX8nEIeTv4egY+nE9zxOw70Daf9+OPN95PPlTIjx/ky0lg6Sc20vzlvXzBgosnH6L5Xb+9kebXLuDrVfzbR/+J5l/d/2ma/23FqzR/uuDsyGxzTvRb8dgi4O6vAYh6dvj/ShHJeBo2LBI4FQGRwKkIiARORUAkcCoCIoFTERAJXErXEyhJ9GP5lN2R+b3b+L4BuTl8XfwLaw7QfH5+A82njef7w1dO4f3Ir3adS/NxNfz+s8/l8+3zV/PpGT1dvB++tJn38/eV8376uPUMvJivV+A7eft6qmLm+8f8yvr1xvn8GxK8/Rtap9K8ekr0WhgA8Mx2/vw/t3sezWdUHqX5jds+R/MvzHgtMrNTj+oHoCsBkeCpCIgETkVAJHAqAiKBUxEQCZyKgEjgVAREApfScQJDnoX2weg57Wtqf0iPv+wf76T5vL/aSvM7H/2PNK/56D6aH+vn8/GHJ/B+/q5tfD2B9mLej11SxPvxE0f409kxi8YYyYsZB5DL+/EnvcLHAbTxbnTk1XTQvPQxvqBAx19207y7oYTmR4b4vgpfvOQlmmdP5efniYbzaf7V6b+i+W11q2j+zbWfiMwauw9GZroSEAmcioBI4FQERAKnIiASOBUBkcCpCIgETkVAJHApHSeQyBrGxNzOyPxLh66lx1etqKf53u6JNO+r4usRDI3wmrj/yASaL57N1xv4g0+neWEp36ato4Dv4DR+YvS5BYBjTdF7PgBATht/OcxbxM//jpIqmpeXddF8TtkRmi//5k6a/+jAJTRfcel2mq9tnUHzg70VNH+rl5/fo118nMkTxy6geXVFO83/3ZQ/RGZ/XxQ9BkNXAiKBUxEQCZyKgEjgVAREAqciIBI4FQGRwKkIiAQudpyAmVUDeBRAJQAH8IC7f9/M7gHwBQCtyW+9292fYffV1l2Ex9ZdFJnffsmvaVueb+Hryv92M99//ool22iek8XXzS9O9NO8oYvPR79k7l6a72nn/dADBXy+/nDMOIeFc3k//5aNfBxDlvH58tfM3ULzf9tyHs1nlPJ1/ftH+M/fP8hfzqu3LaI5WZofAFCQw9eL+OzU12n+rQY+DqYga4Dml0/aRfN/bVgSmR0bjH7tj2aw0BCAr7j7ejMrAbDOzF5IZv/g7t8ZxX2ISIaKLQLu3gigMfl1p5ltB8C3ahGRD4z39JmAmc0AsBjA2uRNt5nZJjN72MzGn+G2iUgKjLoImFkxgF8AuMPdOwDcD2A2gEU4caVwX8Rxt5hZnZnVDXfxNeBEJPVGVQTMLIETBeAn7r4aANy92d2H3X0EwIMATvmJn7s/4O617l6bXVx0ptotImdIbBEwMwPwEIDt7v7dk26ffNK33QCAfzQsIhlpNL0DSwHcBGCzmW1I3nY3gFVmtggnOlYOALj1fWmhiLyvRtM78BqAUy14T8cEnEp2YhjjJ0XPeX/pyDn0+Lj5/HPmNNH85bULaP6Ri/m+Betf5+MQ7rzmaZp/98nraT44kfdDF5T10fxTMzfR/Jl6vvB/wVQ+33/Lft4pdMWFO2h+zvRGml9Qytdj2NBZTfOvzOXjTL69/eM0z8nm40T2NvH1KraWn0XzOxe9QPN1nXycxuqN0eMAAGD5/Oj1FrZmRa+loRGDIoFTERAJnIqASOBUBEQCpyIgEjgVAZHAqQiIBC6l+w6UJPpx2dToOfW3TlhDj79m5+00nzeTjxOoubSV5juPT6J5zHR2rO2YRfPqCxto3tGXT/MLK3k/+q8b59J8esx8/fX7p9H8psV8vvw/rruC5rcteYXmr7fPpPmxfr5uf/s4nn/krD00ry3eT/MdvVNo/tT+hTTv7VlM8zsWv0Tzxtl8vYo1u+dEZp39eZGZrgREAqciIBI4FQGRwKkIiARORUAkcCoCIoFTERAJnLnHLLZ+Jh/MrBXAwZNuqgDAN6VPL7VvbDK5fZncNuDMt2+6u59yQYSUFoE/e3CzOnevTVsDYqh9Y5PJ7cvktgGpbZ/eDogETkVAJHDpLgIPpPnx46h9Y5PJ7cvktgEpbF9aPxMQkfRL95WAiKSZioBI4FQERAKnIiASOBUBkcD9P9+CDFrSTqg5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}